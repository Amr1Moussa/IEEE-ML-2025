{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8eef6f5",
   "metadata": {},
   "source": [
    "# Healthcare-Dataset-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fafe36",
   "metadata": {},
   "source": [
    "*`Imports`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f1179",
   "metadata": {},
   "source": [
    "*`load data`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use os to join the current working directory with the file path\n",
    "FILE_PATH = os.path.join(os.getcwd(), '..', 'data', 'healthcare-dataset-stroke-data.csv')\n",
    "df = pd.read_csv(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f38888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ec7f9",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e74314",
   "metadata": {},
   "source": [
    "- The Stroke Prediction Dataset contains information about patients and various health-related attributes.\n",
    "- The goal is to predict whether a patient is likely to experience a stroke based on these attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d59ce",
   "metadata": {},
   "source": [
    "### Problem Defination "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ad0ee",
   "metadata": {},
   "source": [
    "- Stroke is one of the leading causes of death and disability. Early prediction of stroke risk can help in taking preventive measures. \n",
    "- The aim of this analysis is to build a predictive model that accurately classifies whether a person will experience a stroke based on their medical and demographic data.\n",
    "- We will start with EDA amd preprocesssing of the dataset in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1787fb2",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc22d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the column names to be more user-friendly\n",
    "df.columns = df.columns.str.replace(' ', '_').str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id \n",
    "id_series = df['id']           # Save the 'id' column as a separate Series -> may use later in mapping the predictions back to the original data\n",
    "df.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check of the data types and non-null counts\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values[missing_values > 0]         # will handle later \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for numerical columns\n",
    "df.describe(include='number').T\n",
    "# max far from 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for categorical columns\n",
    "df.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges of the data and possible values for categorical columns\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "    else:\n",
    "        print(f\"{col}: min={df[col].min()}, max={df[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "print(\"Number of duplicate rows:\", len(duplicates))\n",
    "# Display the duplicate rows if exists\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e28eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying mode, median, and mean for all numeric columns\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    mode = df[col].mode()[0] if not df[col].mode().empty else 'No mode'\n",
    "    median = df[col].median()\n",
    "    mean = df[col].mean()\n",
    "    print(f\"Column: {col}\\nMode: {mode}\\nMedian: {median}\\nMean: {mean}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable\n",
    "df['stroke'].value_counts(normalize=True).plot(kind='bar', color=['#1f77b4', '#ff7f0e'])\n",
    "plt.title('Distribution of Stroke Cases')\n",
    "\n",
    "\n",
    "# stroke=0: No stroke, stroke=1: Stroke\n",
    "# stroke=1 is the positive class (required for prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb785fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features distribution alone\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != 'stroke':\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(df[col], kde=True, bins=30, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615c029",
   "metadata": {},
   "source": [
    "*`Bivariate Analysis`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c28b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plots(features vs target)\n",
    "for select in [df[['age', 'avg_glucose_level', 'bmi']]]:\n",
    "    for col in select.columns:\n",
    "        if col != 'stroke':\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.scatterplot(data=df, x=col, y='stroke', alpha=0.5)\n",
    "            plt.title(f'Scatter plot of {col} vs Stroke')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Stroke')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc232d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix for numerical features\n",
    "correlation_matrix = df.select_dtypes(include=['number']).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5f9b5",
   "metadata": {},
   "source": [
    "`Outliers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']]\n",
    "\n",
    "outliers_iqr = pd.DataFrame(False, index=numerical_df.index, columns=numerical_df.columns)\n",
    "\n",
    "for col in numerical_df:\n",
    "    Q1, Q3 = np.percentile(numerical_df[col].dropna(), [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    outliers_iqr[col] = (numerical_df[col] < lower) | (numerical_df[col] > upper)\n",
    "\n",
    "print(\"Outliers per column (IQR):\")\n",
    "print(outliers_iqr.sum())\n",
    "# ratio of outliers\n",
    "print(\"Outlier ratio per column (IQR):\")\n",
    "print(outliers_iqr.mean())\n",
    "# ratio from all\n",
    "print(\"Overall outlier ratio (IQR):\")\n",
    "print(outliers_iqr.values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use boxplot to visualize outliers\n",
    "plt.boxplot(numerical_df.values, labels=numerical_df.columns)\n",
    "plt.title(\"Boxplot of Data\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be119ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score \n",
    "z_scores = zscore(numerical_df)\n",
    "threshold = 3  # confidence is 99.7% (±3 standard deviations)\n",
    "outliers_z = numerical_df[np.abs(z_scores) > threshold]\n",
    "print(\"Z-score Outliers:\")\n",
    "print(outliers_z.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76aa89",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b0b5e",
   "metadata": {},
   "source": [
    "*`Drop id`* done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f70755",
   "metadata": {},
   "source": [
    "*`Encoding`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# List of columns to encode\n",
    "columns = ['gender', 'ever_married', 'work_type', 'smoking_status', 'residence_type']  \n",
    "\n",
    "# Dictionary to store encoders(use for decoding later) and mappings\n",
    "label_encoders = {}\n",
    "mappings = {}\n",
    "\n",
    "# Encoding and creating mappings in a single loop\n",
    "for col in columns:\n",
    "    encoder = LabelEncoder()\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "    label_encoders[col] = encoder\n",
    "\n",
    "    # Creating a dictionary of original: encoded pairs\n",
    "    mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    mappings[col] = mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chech after encoding\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43807df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the encoded values back to original values\n",
    "print(\"\\nOriginal to Encoded Mappings:\")\n",
    "for col, mapping in mappings.items():\n",
    "    print(f\"{col}: {mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6e7e2",
   "metadata": {},
   "source": [
    "*`handle missing values`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the percentage of missing values in each column\n",
    "missing_percentage = df.isnull().mean() * 100\n",
    "missing_percentage[missing_percentage > 0]  # Display columns with missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6626558",
   "metadata": {},
   "source": [
    "outliers handling firstly,impute missing values at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bebaf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values using k-nearest neighbors\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)  # You can adjust n_neighbors based on your data\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)    # in-place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values left after imputation\n",
    "\n",
    "missing_values_after_imputation = df.isnull().sum()\n",
    "missing_values_after_imputation[missing_values_after_imputation > 0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46728c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83ea80",
   "metadata": {},
   "source": [
    "*`Scaling`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d196c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if scaling is needed \n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "    else:\n",
    "        print(f\"{col}: min={df[col].min()}, max={df[col].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarization for logestic regression later\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(df[['age', 'avg_glucose_level', 'bmi']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the scaled values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b497e1",
   "metadata": {},
   "source": [
    "*`check on gender`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b41a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where gender is 2\n",
    "filtered_df = df[df['gender'] == 2]\n",
    "\n",
    "# Count occurrences of stroke = 1 and stroke = 0\n",
    "stroke_1_count = (filtered_df['stroke'] == 1).sum()\n",
    "stroke_0_count = (filtered_df['stroke'] == 0).sum()\n",
    "\n",
    "print(f\"Number of rows where gender is 2 and stroke = 1: {stroke_1_count}\")\n",
    "print(f\"Number of rows where gender is 2 and stroke = 0: {stroke_0_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1936ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 1 row with gender=2(other), so drop this column\n",
    "\n",
    "df = df.drop(filtered_df.index, axis=0)  # Drop the rows where\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10567735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's binary feature\n",
    "# check gender possible values  \n",
    "\n",
    "print(df['gender'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "for col in ['age', 'avg_glucose_level', 'bmi']:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# power transformation for skewed features\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df[['avg_glucose_level', 'bmi']] = pt.fit_transform(df[['avg_glucose_level', 'bmi']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eff5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution after power transformation\n",
    "for col in ['avg_glucose_level', 'bmi']:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[col], kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {col} after Power Transformation')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9014ee",
   "metadata": {},
   "source": [
    "`Outliers after Transformations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8258da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = df[['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']]\n",
    "\n",
    "outliers_iqr = pd.DataFrame(False, index=numerical_df.index, columns=numerical_df.columns)\n",
    "\n",
    "for col in numerical_df:\n",
    "    Q1, Q3 = np.percentile(numerical_df[col].dropna(), [25, 75])\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "    outliers_iqr[col] = (numerical_df[col] < lower) | (numerical_df[col] > upper)\n",
    "\n",
    "print(\"Outliers per column (IQR):\")\n",
    "print(outliers_iqr.sum())\n",
    "# ratio of outliers\n",
    "print(\"Outlier ratio per column (IQR):\")\n",
    "print(outliers_iqr.mean())\n",
    "# ratio from all\n",
    "print(\"Overall outlier ratio (IQR):\")\n",
    "print(outliers_iqr.values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score \n",
    "z_scores = zscore(numerical_df)\n",
    "threshold = 3  # confidence is 99.7% (±3 standard deviations)\n",
    "outliers_z = numerical_df[np.abs(z_scores) > threshold]\n",
    "print(\"Z-score Outliers:\")\n",
    "print(outliers_z.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65132bca",
   "metadata": {},
   "source": [
    "- normalization using power transformation reduced no of outliers significantly\n",
    "- now can remove extreme outliers as their percentage is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0904097",
   "metadata": {},
   "source": [
    "`handle outliers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers with z-score\n",
    "threshold = 3  # confidence is 99.7% (±3 standard deviations)\n",
    "\n",
    "# compute z-scores for numeric columns\n",
    "z_scores = np.abs(zscore(df[numerical_df.columns]))\n",
    "\n",
    "# mask of rows to keep\n",
    "mask = (z_scores <= threshold).all(axis=1)\n",
    "\n",
    "# size before\n",
    "size_before = df.shape[0]\n",
    "\n",
    "# apply mask to the full dataframe (not just numerical_df)\n",
    "df = df[mask].reset_index(drop=True)\n",
    "\n",
    "# size after\n",
    "size_after = df.shape[0]\n",
    "\n",
    "print(f\"Removed {size_before - size_after} outliers (Z-score)\")\n",
    "print(f\"New size: {size_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce70c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to CSV\n",
    "output_file_path = os.path.join(os.getcwd(), '..', 'data','processed_stroke_data.csv')\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12717b",
   "metadata": {},
   "source": [
    "### `Final Df`\n",
    "- Unnecessary dropped\n",
    "- Encoded using label encoding\n",
    "- Missing values imputed with KNN\n",
    "- Scaled using StandardScaler\n",
    "- Outliers handled using power transfotmation\n",
    "- class imbalance -> later after spliting in modeling notebook\n",
    "#### Ready for Logestic Regression or any other clf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv(output_file_path)\n",
    "processed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5493cfc",
   "metadata": {},
   "source": [
    "`prepare data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_df.drop(columns=['stroke'])\n",
    "y = processed_df['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9e6eb",
   "metadata": {},
   "source": [
    "`data splitting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc54a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "# check the shape of the data\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecf483",
   "metadata": {},
   "source": [
    "`handle class imbalance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample the minority class using SMOTE\n",
    "sm = SMOTE(\n",
    "    sampling_strategy=0.5,   # how much to oversample\n",
    "    random_state=42,           \n",
    "    k_neighbors=4,              # number of neighbors to generate new samples\n",
    ")\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "# check shape after oversampling\n",
    "print(\"Training data shape after oversampling:\", X_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the class distribution\n",
    "sns.countplot(x=y_train_res)\n",
    "plt.title(\"Training class distribution after SMOTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76084de5",
   "metadata": {},
   "source": [
    "`train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623950e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression (default)\n",
    "log_reg = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0,             # regularization strength (inverse)\n",
    "    solver=\"lbfgs\"     # you can change to \"liblinear\", \"saga\" etc.\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression (with class_weight balanced)\n",
    "log_reg_balanced = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0,\n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "log_reg_balanced.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a90792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Hard-Margin SVM (linear) -> C is very large (no slack variables)\n",
    "svm_hard = SVC(\n",
    "    kernel=\"linear\",\n",
    "    C=1e6,            # big C → hard margin\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_hard = LinearSVC(C=1e4, random_state=42, max_iter=10000)\n",
    "\n",
    "svm_hard.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Soft-Margin SVM (linear)\n",
    "svm_soft = SVC(\n",
    "    kernel=\"linear\",\n",
    "    C=1.0,            # regularization parameter\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_soft.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62057392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RBF Kernel SVM\n",
    "svm_rbf = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0,            # regularization\n",
    "    gamma=\"scale\",    # kernel coefficient (\"scale\", \"auto\" or float)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_rbf.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Polynomial Kernel SVM\n",
    "svm_poly = SVC(\n",
    "    kernel=\"poly\",\n",
    "    C=1.0,\n",
    "    degree=5,         # degree of polynomial\n",
    "    gamma=\"scale\",    # kernel coefficient\n",
    "    coef0=0.0,        # independent term\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "svm_poly.fit(X_train_res, y_train_res)  # train on SMOTE-resampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af77e65",
   "metadata": {},
   "source": [
    "`evalute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the models with parameters you can adjust\n",
    "models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Logistic Regression (balanced)\": log_reg_balanced,\n",
    "\n",
    "    \"SVM Hard-Margin (Linear)\": svm_hard,\n",
    "    \n",
    "    \"SVM Soft-Margin (Linear)\": svm_soft,\n",
    "\n",
    "    \"SVM RBF Kernel\": svm_rbf,\n",
    "    \"SVM Polynomial Kernel\": svm_poly\n",
    "}\n",
    "\n",
    "# Loop through and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)       # predict on test set\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "# Show results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2D = pca.fit_transform(X_train_res)  # use resampled training data\n",
    "X_test_2D = pca.transform(X_test)\n",
    "\n",
    "# Re-train two models on reduced features\n",
    "svm_linear = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
    "svm_rbf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "\n",
    "svm_linear.fit(X_train_2D, y_train_res)\n",
    "svm_poly.fit(X_train_2D, y_train_res)\n",
    "\n",
    "models_2D = {\"Linear SVM\": svm_linear, \"Poly SVM\": svm_poly}\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", s=20, cmap=plt.cm.coolwarm)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for both models\n",
    "for name, model in models_2D.items():\n",
    "    plot_decision_boundary(model, X_train_2D, y_train_res, f\"Decision Boundary - {name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
